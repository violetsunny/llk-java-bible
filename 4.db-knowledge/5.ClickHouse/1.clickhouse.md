## ClickHouse
ClickHouse是俄罗斯的Yandex于2016年开源的列式存储数据库操作系统（DBMS），使用 C++ 语言编写，主要用于在线分析处理查询（OLAP），能够使用SQL查询实时生成分析数据报告。

ClickHouse由雏形发展至今一共 经历了四个阶段。它的初始设计目标是服务自己公司的一款名叫 Yandex.Metrica的产品。Metrica是一款Web流量分析工具，基于前方探针采集行为数据，然后进行一系列的数据分析，类似数据仓库的 OLAP分析。而在采集数据的过程中，一次页面click（点击），会产生一个event（事件）。至此，整个系统的逻辑就十分清晰了，那就是基 于页面的点击事件流，面向数据仓库进行OLAP分析。所以ClickHouse 的全称是Click Stream，Data WareHouse，简称ClickHouse。

列式存储结构化数据，表的形式组织数据，表有明确的列定义和数据类型。它支持多种常见的数据类型，如整数、浮点数、字符串、日期时间等。
以下表为例



采用行式存储时，数据在磁盘上的组织结构如下，好处是想查某个人所有的属性时，可以通过一次磁盘查找加顺序读取就可以。但是当想查所有人的年龄时，需要不停的查找，或者全表扫描才行，遍历的很多数据都是不需要的。



采用列式存储时，数据在磁盘上的组织结构如下，这时想查所有人的年龄只需把年龄那一列拿出来就可以了。



列式储存的好处：
1）对于列的聚合、计数、求和等统计操作原因优于行式存储。
2）由于某一列的数据类型都是相同的，针对于数据存储更容易进行数据压缩，每一列选择更优的数据压缩算法，大大提高了数据的压缩比重。
3）由于数据压缩比更好，一方面节省了磁盘空间，另一方面对于 cache 也有了更大的发挥空间。

ClickHouse和MySQL类似，把表级的存储引擎插件化，根据表的不同需求可以设定不同的存储引擎。目前包括合并树、日志、接口和其他四大类 20 多种引擎。

向量化执行
为了高效的使用CPU，数据不仅仅按列存储，同时还按向量(列的一部分)进行处理，这样可以更加高效地使用CPU。
为了实现向量化执行，需要利用CPU的SIMD指令。SIMD的全称是Single Instruction Multiple Data，即用单条指令操作多条数据。现代计算机系统概念中，它是通过数据并行以提高性能的一种实现方式（其他的还有指令级并行和线程级并行），它的原理是在CPU寄存器层面实现数据的并行操作，SIMD被广泛地应用于文本转换、数据过滤、数据解压和JSON转换等场景。ClickHouse目前利用SSE4.2指令集实现向量化执行。

高吞吐写入能力
ClickHouse 采用类LSM Tree的结构，数据写入后定期在后台Compaction（压缩）。通过类LSM tree 的结构，ClickHouse 在数据导入时全部是顺序 append写，写入后数据段不可更改，在后台 compaction 时也是多个段 merge sort 后顺序写回磁盘。顺序写的特性，充分利用了磁盘的吞吐能力，即便在 HDD 上也有着优异的写入性能。 官方公开 benchmark测试显示能够达到 50MB-200MB/s的写入吞吐能力，按照每行 100Byte 估算，大约相当于50W-200W条/s的写入速度。

数据分区与线程级并行
ClickHouse 将数据划分为多个 partition（分区），每个partition再进一步划分为多个index granularity(索引粒度)，然后通过多个 CPU核心分别处理其中的一部分来实现并行数据处理。 在这种设计下，单条 Query 就能利用整机所有 CPU。极致的并行处理能力，极大的降低了查询延时。 所以，ClickHouse 即使对于大量数据的查询也能够化整为零平行处理。但是有一个弊端就是对于单条查询使用多cpu，就不利于同时并发多条查询。所以对于高qps的查询业务， ClickHouse并不是强项。

适用场景
因为ClickHouse在诞生之初是为了服务Yandex自家的Web流量分析 产品Yandex.Metrica，所以在存储数据超过20万亿行的情况下， ClickHouse做到了90%的查询都能够在1秒内返回的惊人之举。随后， ClickHouse进一步被应用到Yandex内部大大小小数十个其他的分析场景中。可以说ClickHouse具备了人们对一款高性能OLAP数据库的美好向往，所以它基本能够胜任各种数据分析类的场景，并且随着数据体量的增大，它的优势也会变得越为明显。ClickHouse非常适用于商业智能领域（也就是我们所说的BI领域），除此之外，它也能够被广泛应用于广告流量、Web、App流量、 电信、金融、电子商务、信息安全、网络游戏、物联网等众多其他领域。

不适用场景
ClickHouse作为一款高性能OLAP数据库，虽然足够优秀，但也不是万能的。我们不应该把它用于任何OLTP事务性操作的场景，因为它有以下几点不足。
1）不支持事务。
2）不擅长根据主键按行粒度进行查询（虽然支持），故不应该把 ClickHouse当作Key-Value数据库使用。
3）缺少高频率，低延迟的修改或删除已存在数据的能力，不擅长按行删除数据（虽然支持），仅能用于批量删除或修改数据。
这些弱点并不能视为ClickHouse的缺点，事实上其他同类高性能 的OLAP数据库同样也不擅长上述的这些方面。因为对于一款OLAP数据库而言，上述这些能力并不是重点，只能说这是为了极致查询性能所做的权衡。

## ClickHouse引擎
ClickHouse 非常适合对查询性能要求极高、数据量大且需要进行复杂分析的场景。但不支持事务，不擅长按照行处理数据。
速度快：在面对 1000 万数据集时，比 Hive 快 279 倍，比 MySQL 快 801 倍；
向量化引擎：实现数据级并行处理，将数据放在CPU寄存器，处理更快。
multi-master 多主架构：异步多主复制：数据在多个节点之间进行异步复制，确保每个节点上的数据最终是一致的。

分区存储查询速度更快。指定表数据分区方式，支持多个列，但单个列分区查询效果最好。有数据写入时属于同一分区的数据最终会被合并到同一个分区目录，不同分区的数据永远不会被合并在一起。结合业务场景设置合理的分区可以减少查询时数据文件的扫描范围。
在一个数据片段内，数据以何种方式排序。当使用多个字段排序时ORDER BY(T1,T2)，先按照T1排序，相同值再按照T2排序。

ReplicateMergeTree为有副本的引擎，副本复制是以ZK的节点监听为基础实现，插入数据会Pull通知数据变更存放在log节点下的实体中，然后其他节点Fetching通过http的方式请求数据复制。数据会按照默认8192条数据切分获取。
MergeTree（合并树），MergeTree系列引擎是最基础的表引擎，提供了主键索引、数据分区等基本能力，分区下具体的数据文件包括一级索引、每列压缩文件、每列字段标记文件。
1.一级索引文件，存放稀疏索引，通过ORDER BY或PRIMARY KEY声明，使用少量的索引能够记录大量数据的区间位置信息，内容生成规则跟排序字段有关，且索引数据常驻内存，取用速度快。借助稀疏索引，可以排除主键范围外的数据文件，从而有效减少数据扫描范围，加速查询速度。
2.每列压缩数据文件，存储每一列的数据，每一列字段都有独立的数据文件。
3.每列字段标记文件，每一列都有对应的标记文件，保存了列压缩文件中数据的偏移量信息，与稀疏索引对齐，又与压缩文件对应，建立了稀疏索引与数据文件的映射关系。不能常驻内存，使用LRU缓存策略加快其取用速度。

ReplacingMergeTree实现数据去重
在建表时设置ORDER BY排序字段作为判断重复数据的唯一键，在合并分区的时候会触发删除重复数据，能够一定程度上解决数据重复的问题。

AggregatingMergeTree
在合并分区的时候按照定义的条件聚合数据，将需要聚合的数据预先计算出来，在聚合查询时直接使用结果数据，以空间换时间的方法提高查询性能。该引擎需要使用AggregateFunction类型来处理所有列。

分区字段不易过多，分区过多就意味着数据目录更加复杂，在进行聚合计算时，肯定会影响整个集群的查询性能。目前我们遇到的业务场景，适合以时间字段（时分秒）来作为分区字段，toYYYYMMDD(ts)。
数据会按照设置的排序字段先后顺序来进行存储，在进行聚合计算时也会按照聚合条件对相邻数据进行计算，但如果聚合条件不在排序字段里，集群会对当前分区的所有数据扫描一遍，这种查询就会慢很多，大量消耗集群的内存、CPU资源。我们应该避免这种情况出现，设置合理的排序规则才能以最快的速度聚合出我们想要的结果。
查询优化：通过建立物化视图，以空间换取时间，大部分聚合查询速度能提高10几倍。避免明细数据join。Clickhouse更适合大宽表数据聚合查询，对于明细数据join的场景尽量避免出现。集群硬件升级等。

## CK架构
集群架构
ClickHouse采用多主节点，节点组成集群，集群是副本和分片的基础，它将ClickHouse的服务拓扑由单节点延伸到多个节点，ClickHouse的集群配置非常灵活，用户既可以将所有节点组成一个单一集群，也可以按照业务的诉求，把节点划分为多个小的集群。在每个小的集群区域之间，它们的节点、分区和副本数量可以各不相同。



分片和副本
分片：数据分片是将数据进行横向切分，各个分片数据不同




副本：给数据提供冗余，各个副本数据相同


区别：从数据层面区分，分片之间的数据是不同的，而副本之间的数据是完全相同的。从功能作用层面区分，使用副本的主要目的是防止数据丢失，增加数据存储的冗余；而使用分片的主要目的是实现数据的水平切分。（像多主多从架构，即多主数据不同为各个切片数据，但是从数据相同为主备份）





ClickHouse 采用 multi-master 多主架构，即多个节点都是平等的主节点，没有严格意义上的从节点。在这种架构下，每个节点都可以接受写入和查询请求，并且数据会在多个节点之间进行异步复制。没有单点故障，易于扩展，负载均衡。异步多主复制：数据在多个节点之间进行异步复制，确保每个节点上的数据最终是一致的。数据校验和：ClickHouse 使用数据校验和来检测数据在复制过程中的损坏或错误，并自动进行修复，确保数据的完整性。

ZooKeeper的作用
集群部署需要包含一套ZooKeeper，副本功能依赖ZooKeeper，主副本选举、副本状态感知、操作日志分发、任务队列和BlockID去重判断等，在执行INSERT数据写入、MERGE分区和MUTATION操作的时候，都会涉及与ZooKeeper的通信。但是在通信的过程中，并不会涉及任何表数据的传输，在查询数据的时候也不会访问ZooKeeper，所以不必过于 担心ZooKeeper的承载压力。

分布式表
ClickHouse提供了本地表（Local Table）与分布式表 （Distributed Table）的概念。一张本地表等同于一份分片后的存储数据。 而分布式表本身不存储任何数据，它是本地表的访问代理，其作用类似分库中间件。查询数据需要借助分布式表，能够代理访问多个数据分片，从而实现分布式查询。

## ClickHouse 与 ES
ClickHouse 在大规模数据的复杂分析和处理方面性能卓越，查询速度快，数据压缩比高，适合对数据处理性能和分析能力要求高的场景；ES 在全文搜索和实时性方面表现突出，易于使用和管理，适用于对文本搜索和实时数据分析要求高的场景。

## ClickHouse 与 Hive
Hive也是用于大规模数据分析。在实时查询和分析中CK比Hive更快更高效，在写入存储性能上CK更有优势，在小规模数据和硬件有限的情况下也是CK更有优势。Hive的优势是有完善的生态支撑，和在复杂查询有更多函数支撑。

