## zk工作原理和基本特点
ZK是一个分布式协调系统，分布式锁，集群领导选举，共享计数器，缓存机制，分布式队列。

ZooKeeper 树中的每一层级用斜杠（/）分隔开，且只能用绝对路径（如get /work/task）的方式查询 ZooKeeper 节点，而不能使用相对路径。使用的HashTableConcurrentHashMap<String, DataNode> nodes，用节点的完整路径来作为 key 存储节点数据。

Zookeeper 的核心是原子广播，这个机制保证了各个 Server 之间的同步。实现这个机制的协议叫做 Zab 协 议。Zab 协议有两种模式，它们分别是崩溃恢复模式(选主)和广播模式(同步)。当服务启动或者在master崩溃后，Zab 就进入了恢复模式，当master被选举出来，且大多数 Server 完成了和master的状态同步以后，恢复模式就结束了。状态同步保证了 master 和 Server 具有相同的系统状态，保证服务的一致性。节点接收写请求会转发给leader进行操作，生成zxid保证顺序一致性，并发给所有follower一个提案，大多数follwer响应以后会提交。这里请注意 ，与二阶段提交过程不同（即不需要集群中所有服务器都反馈可以执行事务操作后，主服务器再次发送 commit 提交请求执行数据变更） ，ZAB 协议算法省去了中断的逻辑，当 ZooKeeper 集群中有大多数（超过一半）的 Follow 服务器能够正常执行事务操作后，整个 ZooKeeper 集群就可以提交 Proposal 事务了。

- 顺序性
client的update请求都会根据他发出的顺序被顺序的处理，zxid
- 原子性
一个update操作要么成功要么失败，没有其他可能的结果.
- 强一致性
client无论连接到那个server，展示给它的都是同一个视图
- 可靠性
update一旦成功，就被持久化了，除非另一个update请求更
新了当前值，做消息队列不会丢失消息
- 实时性
对于每一个client，它的系统视图都是最新的。可以进行实时快速复制

## ZK有序性和一致性
客户端的读请求可以被集群中的任意一台机器处理，如果读请求在节点上注册了监听器，这个监听器也是由所 连接的 zookeeper 机器来处理。
对于写请求，这些请求会同时发给其他 zookeeper 机器并且达成一致后，请求才会返回成功。

因此，随着 zookeeper 的集群机器增多，读请求的吞吐会提高但是写请求的吞吐会下降。 有序性是 zookeeper 中非常重要的一个特性，所有的更新都是全局有序的，每个更新都有一个唯一的时间戳， 这个时间戳称为 zxid(Zookeeper Transaction Id)。而读请求只会相对于更新有序，也就是读请求的返回 结果中会带有这个 zookeeper 最新的 zxid。高 32 位是 epoch(新时代)用来标识 leader 是否发生改变，如果有 新的 leader 产生出来，epoch 会自增，低 32 位用来递增计数，变更后会同步给所有follower。

1. 如果在 ZooKeeper 写请求过程中大多数节点确认了信息，但有个别节点更新失败怎么办？
崩溃恢复机制：会对异常节点在选主后进行同步
同步：ZK节点会通过事务日志和快照的方式恢复到一致的状态。
重试：ZK会对标记异常的节点进行重新确认信息，重试完成未处理的事务，达到一致。
2. 而此时读请求打到这个未成功节点怎么办？
节点会首先检查自己的事务日志，看是否有未处理完的事务。如果有，它会尽快处理这些事务，以尝试同步到最新的数据状态。如果不能达到最新状态，则会尝试从其他节点获取最新数据返回或者返回错误给客户端。如果客户端收到错误，会进行重试从其他节点获取数据。
3. 那如果ZK的写请求大多数节点未能回应确认信息，怎么办？
大部分节点都不能确认信息，则leader节点认为集群出现了问题，暂停事务处理。首先会对各个节点尝试重试，看是否是临时故障。如果恢复则继续进行事务，如果不行会进行标记异常节点。如果大部分节点持续异常，则会触发领导者选举。以达到尽快恢复一致性的状态。

- 一致性，写操作是大多数的机器都一致后才返回成功，zxid事务提交，从同步失败会有重试和恢复机制。是顺序一致性
- 有序性，全局有序，全局ZXID标识，64位，高32位是标识，低32位会递增

## Zookeeper 通知机制
client 端会对某个 znode 建立一个 watcher 事件，当该 znode 发生变化时，这些 client 会收到 zk 的通知， 然后 client 可以根据 znode 变化来做出业务上的改变等。

要注意一点是，我们提到 Watch 具有一次性，所以当我们获得服务器通知后要再次添加 Watch 事件。

## Zookeeper 分布式锁
有了 zookeeper 的一致性和有序性，原子性，锁的问题变得容易。锁服务可以分为两类，一个是保持独占，另一个是控制时序。
对于第一类，我们将 zookeeper 上的一个 znode 看作是一把锁，通过 createznode 的方式来实现。所有客户 端都去创建 /distribute_lock 节点，最终成功创建的那个客户端也即拥有了这把锁。用完删除掉自己创建的 distribute_lock 节点就释放出锁。
对于第二类， /distribute_lock 已经预先存在，所有客户端在它下面创建临时顺序编号目录节点，和选 master 一样，编号最小的获得锁，用完删除，依次执行。其他会监听小于自己编号，等待。zxid和watcher监听。
要避免死锁的问题，服务断开连接要自动删除节点信息。不过持久化顺序节点会有业务服务异常断开连接不删除造成死锁，临时顺序节点会有zk服务重启导致没有锁信息导致锁失败。
避免羊群效应，每个锁请求者 watch 它前面的锁请求者。（羊群效应就是大家都监控同一节点，节点删除通知其他节点时会导致一哄而上抢锁）也就是说ZK是公平锁。

ZK的客户端Curator提供的InterProcessMutex分布式锁的实现
同理可以实现分布式队列

## Zookeeper 四种目录节点
1. 持久化目录
2. 持久化顺序编号目录
3. 临时目录 内存中
4. 临时顺序编号目录 内存中

1. 持久节点会一直存在，临时节点在会话结束后就会删除。
2. 有序节点是自动使用一个单调递增的数字作为后缀，如：works/task1

「节点的状态结构」
执行stat /zk_test，可以看到控制台输出了一些信息，这些就是节点状态信息。
每一个节点都有一个自己的状态属性，记录了节点本身的一些信息：
「状态属性」	「说明」
czxid	数据节点创建时的事务 ID
ctime	数据节点创建时的时间
mzxid	数据节点最后一次更新时的事务 ID
mtime	数据节点最后一次更新时的时间
pzxid	数据节点的子节点最后一次被修改时的事务 ID
「cversion」	「子节点的版本」
「version」	「当前节点数据的版本」
「aversion」	「节点的 ACL 的版本」
ephemeralOwner	如果节点是临时节点，则表示创建该节点的会话的 SessionID；如果节点是持久节点，则该属性值为 0
dataLength	数据内容的长度
numChildren	数据节点当前的子节点个数

## Zookeeper 持久化
1. 合并提交
2. 快照

数据复制：
1. 写主(WriteMaster) :对数据的修改提交给指定的节点。读无此限制，可以读取任何一个节点。这种情况下 客户端需要对读与写进行区别，俗称读写分离;
2. 写任意(Write Any):对数据的修改可提交给任意的节点，跟读一样。这种情况下，客户端对集群节点的角 色与变化透明。
对 zookeeper 来说，它采用的方式是写主。通过增加机器，它的读吞吐能力和响应能力扩展性非常好，而 写，随着机器的增多吞吐能力肯定下降(这也是它建立 observer 的原因)，而响应能力则取决于具体实现方式，是延迟复制保持最终一致性，还是立即复制快速响应。

数据结构：
ZooKeeper 的数据模型可以看作一棵树形结构，而数据节点就是这棵树上的叶子节点。
DataTree 的内部定义类 nodes 节点类型、root 根节点信息、子节点的 WatchManager 监控信息等数据模型中的相关信息。
可以说，一个 DataTree 类定义了 ZooKeeper 内存数据的逻辑结构。

采用了一个 Jute 的序列解决方案作为 ZooKeeper 框架自身的序列化方式。

## ZK会话
数据结构由三个部分组成：分别是会话 ID（sessionID）、会话超时时间（TimeOut）、会话关闭状态（isClosing）。
ZooKeeper 客户端在和服务端建立连接的时候，会提交一个客户端设置的会话超时时间，而该超时时间会和服务端设置的最大超时时间和最小超时时间进行比对，如果正好在其允许的范围内，则采用客户端的超时时间管理会话。
如果大于或者小于服务端设置的超时时间，则采用服务端设置的值管理会话。

客户端所发送的心跳信息可以是一个 ping 请求，也可以是一个普通的业务请求。ZooKeeper 服务端接收请求后，会更新会话的过期时间，来保证会话的存活状态。

处理会话过期的分桶策略：
在 ZooKeeper 中，会话将按照不同的时间间隔进行划分，超时时间相近的会话将被放在同一个间隔区间中，这种方式避免了 ZooKeeper 对每一个会话进行检查，而是采用分批次的方式管理会话。
这就降低了会话管理的难度，因为每次小批量的处理会话过期也提高了会话处理的效率。
ZooKeeper 底层就是采用队列结构来管理会话过期的，一个会话过期队列是由若干个 bucket 组成的。
bucket 是一个按照时间划分的区间，通常以 expirationInterval 为单位进行时间区间的划分，它是 ZooKeeper 分桶策略中用于划分时间区间的最小单位。每个 bucket 中存放了在某一时间内过期的会话。
将会话按照不同的过期时间段分别维护到过期队列之后，首先，ZooKeeper 服务会开启一个线程专门用来检索过期队列，找出要过期的 bucket，而 ZooKeeper 每次只会让一个 bucket 的会话过期，每当要进行会话过期操作时，ZooKeeper 会唤醒一个处于休眠状态的线程进行会话过期操作，之后会按照上面介绍的操作检索过期队列，取出过期的会话后会执行过期操作。

## ZK集群
在 ZooKeeper 集群中将服务器分成 「Leader 、Follow 、Observer 三」种角色服务器，在集群运行期间这三种服务器所负责的工作各不相同：

Leader 角色服务器负责管理集群中其他的服务器，是集群中工作的分配和调度者，既可以为客户端提供写服务又能提供读服务。
Follow 服务器的主要工作是选举出 Leader 服务器，在发生 Leader 服务器选举的时候，系统会从 Follow 服务器之间根据多数投票原则，选举出一个 Follow 服务器作为新的 Leader 服务器，只能提供读服务。
Observer 服务器则主要负责处理来自客户端的获取数据等请求，并不参与 Leader 服务器的选举操作，也不会作为候选者被选举为 Leader 服务器，只能提供读服务。

在 ZooKeeper 集群接收到来自客户端的会话请求操作后，首先会判断该条请求是否是事务性的会话请求。
❝ 对于事务性的会话请求，ZooKeeper 集群服务端会将该请求统一转发给 Leader 服务器进行操作。 所谓事务性请求，是指 ZooKeeper 服务器执行完该条会话请求后，是否会导致执行该条会话请求的服务器的数据或状态发生改变，进而导致与其他集群中的服务器出现数据不一致的情况。 ❞ Leader 服务器内部执行该条事务性的会话请求后，再将数据同步给其他角色服务器，从而保证事务性会话请求的执行顺序，进而保证整个 ZooKeeper 集群的数据一致性。
❝ 在 ZooKeeper 集群的内部实现中，是通过什么方法保证所有 ZooKeeper 集群接收到的事务性会话请求都能交给 Leader 服务器进行处理的呢？ ❞ 在 ZooKeeper 集群内部，集群中除 Leader 服务器外的其他角色服务器接收到来自客户端的事务性会话请求后，必须将该条会话请求转发给 Leader 服务器进行处理。

ZooKeeper 集群中的 Follow 和 Observer 服务器，都会检查当前接收到的会话请求是否是事务性的请求，如果是事务性的请求，那么就将该请求以 REQUEST 消息类型转发给 Leader 服务器。
在 ZooKeeper集群中的服务器接收到该条消息后，会对该条消息进行解析。
分析出该条消息所包含的原始客户端会话请求。
之后将该条消息提交到自己的 Leader 服务器请求处理链中，开始进行事务性的会话请求操作。
如果不是事务性请求，ZooKeeper 集群则交由 Follow 和 Observer 角色服务器处理该条会话请求，如查询数据节点信息。
当一个业务场景在查询操作多而创建删除等事务性操作少的情况下，ZooKeeper 集群的性能表现的就会很好。
❝ 如果是在极端情况下，ZooKeeper 集群只有事务性的会话请求而没有查询操作，那么 Follow 和 Observer 服务器就只能充当一个请求转发服务器的角色， 所有的会话的处理压力都在 Leader 服务器。 ❞ 在处理性能上整个集群服务器的瓶颈取决于 Leader 服务器的性能。

❝ ZooKeeper 集群的作用只能保证在 Leader 节点崩溃的时候，重新选举出 Leader 服务器保证系统的稳定性。 ❞ 这也是 ZooKeeper 设计的一个缺点。

## Zookeeper选举原则
1. 只有超过半数以上的服务器正常启动，集群才能正常工作
2. zxid会给zxid大的进行投票，持续到集群正常工作，选出leader
3. 选出leader后，之前服务器的状态由looking改变为following,以后服务器都是follower

崩溃恢复，重新选举后，跟follower进行数据同步。消息广播模式，非leader会将事务提交首先发送给leader进行生成事务提案，如果过半就会提交。

Leader 服务器的选举操作主要发生在两种情况下：
第一种就是 ZooKeeper 集群服务启动的时候，第二种就是在 ZooKeeper 集群中旧的 Leader 服务器失效时。

事务ID（ZXID）标识由Leader统一分配，全局唯一，长度64位，递增。如果zxid1小于zxid2，则说明zxid1的事务在zxid2的事务之前发生。
「选举过程」
在 ZooKeeper 集群重新选举 Leader 节点的过程中，主要可以分为 Leader 失效发现、重新选举 Leader 、Follow 服务器角色变更、集群同步这几个步骤。
❝ Leader 失效发现 ❞ 在 ZooKeeper 集群中，当 Leader 服务器失效时，ZooKeeper 集群会重新选举出新的 Leader 服务器。
在 ZooKeeper 集群中，探测 Leader 服务器是否存活的方式与保持客户端活跃性的方法非常相似。 首先，Follow 服务器会定期向 Leader 服务器发送 网络请求，在接收到请求后，Leader 服务器会返回响应数据包给 Follow 服务器，而在 Follow 服务器接收到 Leader 服务器的响应后，如果判断 Leader 服务器运行正常，则继续进行数据同步和服务转发等工作，反之，则进行 Leader 服务器的重新选举操作。
❝ Leader重新选举 ❞ 当 Follow 服务器向 Leader 服务器发送状态请求包后，如果没有得到 Leader 服务器的返回信息，这时，如果是集群中个别的 Follow 服务器发现返回错误，并不会导致 ZooKeeper 集群立刻重新选举 Leader 服务器，而是将该 Follow 服务器的状态变更为 LOOKING 状态，并向网络中发起投票，当 ZooKeeper 集群中有更多的机器发起投票，最后当投票结果满足多数原则的情况下。
ZooKeeper 会重新选举出 Leader 服务器。
❝ Follow 角色变更 ❞ 在 ZooKeeper 集群中，Follow 服务器作为 Leader 服务器的候选者，当被选举为 Leader 服务器之后，其在 ZooKeeper 集群中的 Follow 角色，也随之发生改变。也就是要转变为 Leader 服务器，并作为 ZooKeeper 集群中的 Leader 角色服务器对外提供服务。
❝ 集群同步数据 ❞ 在 ZooKeeper 集群成功选举 Leader 服务器，并且候选 Follow 服务器的角色变更后。
为避免在这期间导致的数据不一致问题，ZooKeeper 集群在对外提供服务之前，会通过 Leader 角色服务器管理同步其他角色服务器。

logicClock：用来记录服务器的投票轮次。logicClock 会从 1 开始计数，每当该台服务经过一轮投票后，logicClock 的数值就会加 1 。
state：用来标记当前服务器的状态。在 ZooKeeper 集群中一台服务器具有 LOOKING、FOLLOWING、LEADERING、OBSERVING 这四种状态。
self_id：用来表示当前服务器的 ID 信息，该字段在 ZooKeeper 集群中主要用来作为服务器的身份标识符。
self_zxid：当前服务器上所保存的数据的最大事务 ID ，从 0 开始计数。
vote_id：投票要被推举的服务器的唯一 ID 。
vote_zxid：被推举的服务器上所保存的数据的最大事务 ID ，从 0 开始计数。
当 ZooKeeper 集群需要重新选举出新的 Leader 服务器的时候，就会根据上面介绍的投票信息内容进行对比，以找出最适合的服务器。
首先，会对比 logicClock 服务器的投票轮次，当 logicClock 相同时，表明两张选票处于相同的投票阶段，并进入下一阶段，否则跳过。
接下来再对比vote_zxid被选举的服务器 ID 信息，若接收到的外部投票信息中的 vote_zxid字段较大，则将自己的票中的vote_zxid与vote_id更新为收到的票中的vote_zxid与vote_id，并广播出去。
要是对比的结果相同，则继续对比vote_id，若外部投票的vote_id 比较大，则将自己的票中的 vote_id更新为收到的票中的vote_id。。
经过这些对比和替换后，最终该台 Follow 服务器会产生新的投票信息，并在下一轮的投票中发送到 ZooKeeper 集群中。

## Zookeeper选举详细
「底层实现」
首先，ZooKeeper 集群会先判断 Leader 服务器是否失效，而判断的方式就是 Follow 服务器向 Leader 服务器发送请求包，之后 Follow 服务器接收到响应数据后，进行解析，Follow 服务器会根据返回的数据，判断 Leader 服务器的运行状态，如果返回的是 LOOKING 关键字，表明与集群中 Leader 服务器无法正常通信。
之后，在 ZooKeeper 集群选举 Leader 服务器时，是通过 「FastLeaderElection」 类实现的。 该类实现了 TCP 方式的通信连接，用于在 ZooKeeper 集群中与其他 Follow 服务器进行协调沟通。

FastLeaderElection 类继承了 Election 接口，定义其是用来进行选举的实现类。
而在其内部，又定义了选举通信相关的一些配置参数，比如 finalizeWait 最终等待时间、最大通知间隔时间 maxNotificationInterval 等。 在选举的过程中，首先调用 ToSend 函数向 ZooKeeper 集群中的其他角色服务器发送本机的投票信息，其他服务器在接收投票信息后，会对投票信息进行有效性验证等操作，之后 ZooKeeper 集群统计投票信息，如果过半数的机器投票信息一致，则集群就重新选出新的 Leader 服务器。
❝ 这里我们要注意一个问题，那就是在重新选举 Leader 服务器的过程中，ZooKeeper 集群理论上是无法进行事务性的请求处理的。 ❞ 因此，发送到 ZooKeeper 集群中的事务性会话会被挂起，暂时不执行，等到选举出新的 Leader 服务器后再进行操作。

「Observer」
在 ZooKeeper 集群服务运行的过程中，Observer 服务器与 Follow 服务器具有一个相同的功能，那就是负责处理来自客户端的诸如查询数据节点等非事务性的会话请求操作。
但与 Follow 服务器不同的是，Observer 不参与 Leader 服务器的选举工作，也不会被选举为 Leader 服务器。 在早期的 ZooKeeper 集群服务运行过程中，只有 Leader 服务器和 Follow 服务器。
不过随着 ZooKeeper 在分布式环境下的广泛应用，早期模式的设计缺点也随之产生，主要带来的问题有如下几点：
1.随着集群规模的变大，集群处理写入的性能反而下降。
2.ZooKeeper 集群无法做到跨域部署。
其中最主要的问题在于，当 ZooKeeper 集群的规模变大，集群中 Follow 服务器数量逐渐增多的时候，ZooKeeper 处理创建数据节点等事务性请求操作的性能就会逐渐下降。
这是因为 ZooKeeper 集群在处理事务性请求操作时，要在 ZooKeeper 集群中对该事务性的请求发起投票，只有超过半数的 Follow 服务器投票一致，才会执行该条写入操作。
正因如此，随着集群中 Follow 服务器的数量越来越多，一次写入等相关操作的投票也就变得越来越复杂，并且 Follow 服务器之间彼此的网络通信也变得越来越耗时，导致随着 Follow 服务器数量的逐步增加，事务性的处理性能反而变得越来越低。
为了解决这一问题，在 ZooKeeper 3.6 版本后，ZooKeeper 集群中创建了一种新的服务器角色，即 Observer——观察者角色服务器。 Observer 可以处理 ZooKeeper 集群中的非事务性请求，并且不参与 Leader 节点等投票相关的操作。
这样既保证了 ZooKeeper 集群性能的扩展性，又避免了因为过多的服务器参与投票相关的操作而影响 ZooKeeper 集群处理事务性会话请求的能力。
在实际部署的时候，因为 Observer 不参与 Leader 节点等操作，并不会像 Follow 服务器那样频繁的与 Leader 服务器进行通信。 因此，可以将 Observer 服务器部署在不同的网络区间中，这样也不会影响整个 ZooKeeper 集群的性能，也就是所谓的跨域部署。

「在我们日常使用 ZooKeeper 集群服务器的时候，集群中的机器个数应该选择奇数个？」

两个原因：
❝ 在容错能力相同的情况下，奇数台更节省资源 ❞ Zookeeper中 Leader 选举算法采用了Zab协议。
Zab核心思想是当多数 Server 写成功，则写成功。
举两个例子：
假如zookeeper集群1 ，有3个节点，3/2=1.5 , 即zookeeper想要正常对外提供服务（即leader选举成功），至少需要2个节点是正常的。换句话说，3个节点的zookeeper集群，允许有一个节点宕机。
假如zookeeper集群2，有4个节点，4/2=2 , 即zookeeper想要正常对外提供服务（即leader选举成功），至少需要3个节点是正常的。换句话说，4个节点的zookeeper集群，也允许有一个节点宕机。
集群1与集群2都有 允许1个节点宕机 的容错能力，但是集群2比集群1多了1个节点。在相同容错能力的情况下，本着节约资源的原则，zookeeper集群的节点数维持奇数个更好一些。
❝ 防止由脑裂造成的集群不可用。 ❞ 集群的脑裂通常是发生在节点之间通信不可达的情况下，集群会分裂成不同的小集群，小集群各自选出自己的master节点，导致原有的集群出现多个master节点的情况，这就是脑裂。
下面举例说一下为什么采用奇数台节点，就可以防止由于脑裂造成的服务不可用：
假如zookeeper集群有 5 个节点，发生了脑裂，脑裂成了A、B两个小集群：
A ：1个节点 ，B ：4个节点
A ：2个节点， B ：3个节点
可以看出，上面这两种情况下，A、B中总会有一个小集群满足 可用节点数量 > 总节点数量/2 。
所以zookeeper集群仍然能够选举出leader ， 仍然能对外提供服务，只不过是有一部分节点失效了而已。
假如zookeeper集群有4个节点，同样发生脑裂，脑裂成了A、B两个小集群：
A：1个节点 ， B：3个节点
A：2个节点 ， B：2个节点
因为A和B都是2个节点，都不满足 可用节点数量 > 总节点数量/2 的选举条件， 所以此时zookeeper就彻底不能提供服务了。

## ZK客户端
Apache Curator
应用 Apache Curator 来快速实现分布式锁，Curator 是 Netflix 公司开源的一个 ZooKeeper 客户端，对 ZooKeeper 原生 API 做了抽象和封装。

## ZK的ACL权限
一个 ACL 权限设置通常可以分为 3 部分，分别是：权限模式（Scheme）、授权对象（ID）、权限信息（Permission）。
权限就是指我们可以在数据节点上执行的操作种类，在 ZooKeeper 中已经定义好的权限有 5 种：

数据节点（create）创建权限，授予权限的对象可以在数据节点下创建子节点；

数据节点（wirte）更新权限，授予权限的对象可以更新该数据节点；

数据节点（read）读取权限，授予权限的对象可以读取该节点的内容以及子节点的信息；

数据节点（delete）删除权限，授予权限的对象可以删除该数据节点的子节点；

数据节点（admin）管理者权限，授予权限的对象可以对该数据节点体进行 ACL 权限设置。

## ZK应用
分布式ID:顺序节点，不过规则定义依赖于自身。

负载均衡：通过服务节点的订阅信息来做轮询还是随机等操作。

## Quorum机制
是指"法定人数"，指的是在分布式系统中所需要的最少的参与节点的数量或投票的数量。例如如果5个节点，Quorum配置3，则在leader选举投票和写入数据成功节点数时有3个就可以是为成功。

## Nacos
动态服务发现、配置管理和服务管理平台。是通过更友好的RESTful API来实现，更适合作为服务注册中心管理服务。
如果说ZK是CP服务那nacos就是AP服务。

## etcd
高可用的 Key/Value 存储系统，raft协议，可以用restful和grpc，k8s组件

etcd是一个分布式、高可用的键值存储系统，主要用于共享配置和服务发现。它是由CoreOS开发的，基于Raft一致性算法来保证数据的强一致性。
etcd的主要特点如下：
- 强一致性：etcd使用Raft算法来保证数据的一致性，这意味着在多个节点之间的数据同步是可靠的。
- 高可用性：etcd可以部署为多节点集群，当部分节点出现故障时，集群仍然可以正常提供服务。
- 支持TTL：etcd支持设置键值对的生存时间（TTL），在指定时间后键值对会自动过期并删除。
- 支持Watch：etcd允许客户端监视某个键值对的变化，当数据发生变化时，客户端会收到通知。
- 支持事务：etcd支持原子操作，可以在一个事务中执行多个操作。

etcd适用于需要强一致性和高可用性的场景，如配置管理、服务发现和分布式锁等。

