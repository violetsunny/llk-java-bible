## netty
NIO,IO多路复用，零拷贝  就是跳过用户态，从内核态--》socket缓冲区--》网卡

Netty是异步事件驱动框架，其实本质上就是Reactor模式的实现，Selector作为多路复用器，EventLoop作为转发器，Pipeline作为事件处理器。但是和一般的Reactor不同的是，Netty使用串行化实现，并在Pipeline中使用了责任链模式。
Netty中的buffer相对有NIO中的buffer又做了一些优化，大大提高了性能。

Selector选择器进行多个事件监听,通过事件转发器然后进行一个个事件处理机制。一个线程就可以处理很多连接请求。也是IO多路复用器。

主从 Reactor 多线程的设计模式解决了单一 Reactor 的瓶颈。主从 Reactor 职责明确，主 Reactor 只负责监听连接建立事件，SubReactor只负责监听读写事件。整个主从 Reactor 多线程架构充分利用了多核 CPU 的优势，可以支持扩展，而且与具体的业务逻辑充分解耦，复用性高。
模型：
Netty中主Reactor是BossGroup负责监听服务器的连接请求，从Reactor是WorkGroup负责监听连接上的读写事件，可以有多个从Reactor。BossGroup 通常不会有多个 NioEventGroup，而WorkerGroup可以配置多个NioEventGroup，NioEventGroup（如NioEventLoopGroup）包含多个NioEventLoop，NioEventLoopGroup可以看成是个线程池管理。每个NioEventLoop（线程）都包含一个Selector和一个任务队列TaskQueue，其中Acceptor称为acceptor 角色的NioEventLoop。
一个 Channel 对应一个 ChannelPipeline处理器链，每个Channel会通过Selector被注册到NioEventLoop中，ChannelHandler是Channel的具体实现处理，NioEventLoop通过Selector监听Channel上的事件也就是ChannelPipeline处理器链（串行），而具体ChannelHandler是多个（比如ChannelInboundHandler 或 ChannelOutboundHandler）。

请求流转过程：
1. 服务启动后，ServerSocketChannel通过Selector被注册到NioEventLoop（Acceptor）中，所以Acceptor也就会监听这个ServerSocketChannel绑定的服务端口，并指定对OP_ACCEPT事件感兴趣。
2. 当一个Client请求过来后，BossGroup的Acceptor（NioEventLoop线程）负责监听服务器的端口，在有新的连接请求到达时，它会监听到OP_ACCEPT事件（NioEventLoop会不断地从Selector中获取事件，如果有新的连接请求到达，Selector会返回OP_ACCEPT事件）。
3. 监听OP_ACCEPT事件后，Acceptor会调用ServerSocketChannel的accept()方法来接受连接请求，并创建一个新的SocketChannel对象来处理这个连接。
4. 会选择一个WorkerGroup中的NioEventLoop，并将这个SocketChannel注册到这个NioEventLoop的Selector上，后续对这个连接的读写事件进行监听。这个NioEventLoop会不断地从Selector中获取事件建立通道联系。
5. 当客户端发送数据时，SocketChannel会触发OP_READ事件。NioEventLoop会获取到OP_READ事件，并通过ChannelPipeline处理器链进行处理。当有数据可读时，NioEventLoop会调用ChannelInboundHandler中的channelRead()方法来读取数据，并进行相应的业务处理。当需要向客户端发送数据时，NioEventLoop会调用ChannelOutboundHandler中的write()方法来将数据写入到SocketChannel中。这些ChannelPipeline处理器链都是SocketChannel创建通过initChannel()放入socketChannel.pipeline()的。
6. 通过主从Reactor我们明确了各自职责，毕竟读写多连接少，一次连接可以多次读写，不会由于读写阻塞连接。但是只连接不读写会造成WorkGroup空转。
7. 客户端发起关闭连接请求，服务器的SocketChannel会触发OP_READ事件读取到代表关闭的特殊字节序列。当连接关闭后，需要进行资源清理工作，包括释放SocketChannel、缓冲区等资源。（Netty 会针对 HTTP 的连接关闭自动处理）

优点：
入门简单，使用方便，文档齐全，无其他依赖，只依赖 JDK 就够了。
高性能，高吞吐，低延迟，资源消耗少。
灵活的线程模型，支持阻塞和非阻塞的I/O 模型。
代码质量高，目前主流版本基本没有 Bug。

## netty核心组件
1. Channel 主要提供了异步的网络 I/O 操作，例如：建立连接、读写操作等。异步调用意味着任何 I/O 调用都将立即返回，并且不保证在调用返回时所请求的 I/O 操作已完成。I/O 操作返回的是一个 ChannelFuture 对象，无论 I/O 操作是否成功，Channel 都可以通过监听器通知调用方，我们通过向 ChannelFuture 上注册监听器来监听 I/O 操作的结果。
通道类型有以下：
NioSocketChannel： 异步非阻塞的客户端 TCP Socket 连接。
NioServerSocketChannel： 异步非阻塞的服务器端 TCP Socket 连接。
常用的就是这两个通道类型，因为是异步非阻塞的。所以是首选。
OioSocketChannel： 同步阻塞的客户端 TCP Socket 连接。
OioServerSocketChannel： 同步阻塞的服务器端 TCP Socket 连接。

option()设置的是服务端用于接收进来的连接，也就是boosGroup线程。
childOption()是提供给父管道接收到的连接，也就是workerGroup线程。

2. Selector 是对多路复用器的抽象，也是 Java NIO 的核心基础组件之一。Netty 就是基于 Selector 对象实现 I/O 多路复用的，在 Selector 内部，会通过系统调用不断地查询这些注册在其上的 Channel 是否有已就绪的 I/O 事件，例如，可读事件（OP_READ）、可写事件（OP_WRITE）或是网络连接事件（OP_ACCEPT）等，而无须使用用户线程进行轮询。这样，我们就可以用一个线程监听多个 Channel 上发生的事件。
3. ChannelPipeline 之上可以注册多个 ChannelHandler（ChannelInboundHandler 或 ChannelOutboundHandler），我们在 ChannelHandler 注册的时候决定处理 I/O 事件的顺序，这就是典型的责任链模式。一个 Channel 对应一个 ChannelPipeline，一个 ChannelHandlerContext 对应一个ChannelHandler。ChannelPipeline处理器链都是SocketChannel创建通过initChannel()放入socketChannel.pipeline()中。
处理器Handler主要分为两种：
ChannelInboundHandlerAdapter(入站处理器)、ChannelOutboundHandler(出站处理器)
入站指的是数据从底层java NIO Channel到Netty的Channel。
出站指的是通过Netty的Channel来操作底层的java NIO Channel。
4. NioEventLoop 主要做三件事：监听 I/O 事件、执行普通任务以及执行定时任务。
常用的实现类NioEventLoopGroup，OioEventLoopGroup。每个EventLoopGroup里包括一个或多个EventLoop，每个EventLoop中维护一个Selector实例和Task队列。
5. ByteBuf内存容器
ByteBuf池化的性能比非池化性能高8倍多，在API网关，RPC，流式处理器中请求和响应消息是朝升夕灭，频繁申请和释放btye数组对于GC压力比较大，池化的重用对象可以降低GC消耗，同时提升吞吐量。

## netty内存容器
ByteBuf 类似于一个字节数组，其中维护了一个读索引和一个写索引，分别用来控制对 ByteBuf 中数据的读写操作。读索引是为了防止读取越界，写入索引是为了写入后自动扩容或者异常抛出。
Netty 中主要分为以下三大类 ByteBuf：
Heap Buffer（堆缓冲区）。这是最常用的一种 ByteBuf，它将数据存储在 JVM 的堆空间，其底层实现是在 JVM 堆内分配一个数组，实现数据的存储。堆缓冲区可以快速分配，当不使用时也可以由 GC 轻松释放。它还提供了直接访问底层数组的方法，通过 ByteBuf.array() 来获取底层存储数据的 byte[] 。
Direct Buffer（直接缓冲区）。直接缓冲区会使用堆外内存存储数据，不会占用 JVM 堆的空间，使用时应该考虑应用程序要使用的最大内存容量以及如何及时释放。直接缓冲区在使用 Socket 传递数据时性能很好，当然，它也是有缺点的，因为没有了 JVM GC 的管理，在分配内存空间和释放内存时，比堆缓冲区更复杂，Netty 主要使用内存池来解决这样的问题，这也是 Netty 使用内存池的原因之一。
Composite Buffer（复合缓冲区）。我们可以创建多个不同的 ByteBuf，然后提供一个这些 ByteBuf 组合的视图，也就是 CompositeByteBuf。它就像一个列表，可以动态添加和删除其中的 ByteBuf。
Netty 使用 ByteBuf 对象作为数据容器，进行 I/O 读写操作，其实 Netty 的内存管理也是围绕着ByteBuf 对象高效地分配和释放。从内存管理角度来看，ByteBuf 可分为 Unpooled 和 Pooled 两类。

PooledArena:表示内存中一大块连续的区域，PooledArena由多个Chunk组成，每个Chunk由多个Page组成。一般内存中会有多个PooledArena。
PoolChunk:表示对多个Page的管理内存分配和释放。默认大小为 16 MB。
PoolSubPage:对于小于Page的内存，Netty在page中完成分配。每个Page的由大小相等的存储块Region组成，存储块Region的大小由第一次申请Page的时候决定。
Netty 首先会向系统申请一整块连续内存PooledArena，称为 Chunk（默认大小为 16 MB），这一块连续的内存通过 PoolChunk 对象进行封装。之后，Netty 将 Chunk 空间进一步拆分为 Page，每个 Chunk 默认包含 2048 个 Page，每个 Page 的大小为 8 KB。

Netty 的内存分配策略主要包括以下几个方面：
1. 内存规格化：Netty 将申请的内存大小进行规格化处理，即将不同大小的内存请求转换为固定规格的大小，以便于内存的分配和管理。规格化处理涉及到向上取整到最接近的2的幂次方数值 。
2. 内存池化管理：Netty 使用内存池来减少内存分配和回收的开销。内存池通过多个固定大小的 Chunk 进行管理，每个 Chunk 包含多个 Page，Page 是内存分配的基本单位，默认大小为 8KB 。
3. 伙伴算法（Buddy System）：Netty 采用伙伴算法来管理内存分配和回收，特别是对于大于 Page 大小的内存请求。伙伴算法通过将内存分为多个块，并在需要时合并或分割这些块来满足内存请求，从而减少内存碎片 。
4. 本地线程缓存（PoolThreadCache）：为了进一步提高内存分配的效率，Netty 为每个线程提供了一个本地缓存 PoolThreadCache。这个缓存用于存储最近释放的内存块，以便在下次内存请求时可以快速重用 。
5. 不同规格内存的管理：Netty 将内存分为不同的规格，包括 Tiny（小于 512 字节）、Small（介于 512 字节到 8KB 之间）、Normal（大于 8KB 但小于 Chunk 大小）、Huge（大于 Chunk 大小的内存请求） 。
6. 内存分配的弹性伸缩：Netty 的内存池支持弹性伸缩，通过 PoolChunkList 管理多个 PoolChunk，根据内存使用率将它们分类，并在内存需求变化时动态调整 。
7. 内存释放策略：当内存不再需要时，Netty 会将其回收并存储在本地线程缓存中，如果缓存满了，则会将内存块释放回 PooledArena 中。此外，Netty 还提供了内存泄露检测功能，以避免内存泄漏 。
选择合适的配置需要根据应用的具体需求和运行环境来决定。例如，如果应用需要频繁分配和释放小内存块，可以增加 PoolThreadCache 的大小以减少锁竞争。如果应用需要处理大量并发连接，则可能需要增加 PooledArena 的数量以提高并发性能 。

## Vert.x以Netty为基础的框架
Vert.x是事件驱动的，其处理请求的高性能也是基于其事件机制。Vert.x的事件机制中有几个非常重要的概念：Event Loop、Event Loop Vertical、Worker Vertical、Event Bus、Vert.x Module。
Event Loop：即事件循环，是由Vert.x启动的事件处理线程，也是Vert.x项目对外开放的入口，Vert.x由此接收请求事件。一个Vert.x有一个或多个事件循环线程组成，线程最大数量为主机有效的CPU核数。
Event Loop Vertical：事件的业务处理线程，存在于Event Loop中，用于处理非阻塞短任务。
Worker Vertical : 事件的业务处理线程，用于处理长任务阻塞任务。
Event Bus：即事件总线，是Vert.x事件模型中最核心的部分，所有的事件都经由事件总线进行分发，包括Vertical之间的通信事件。
Vert.x Module : Vert.x项目模块，一个应用通常由多个模块组成，每个模块一般包含多个Vertical。

事件模型流程:
Vert.x以非阻塞IO的思想来实现高性能，非阻塞IO的实现，基于Event Loop Vertical和Worker Vertical的分离，在Vert.x中，Event Loop用于接收，并将短业务操作交由其内部的Vertical来处理，该模块是非阻塞的，这样可以保证请求的处理效率；阻塞任务通过Vert.x的事件机制脱离当前线程，转移到Worker Vertical中执行，并执行结果返回给Event Loop Vertical。 这一过程完成的核心是Event Bus，Event Bus中注册了所有的事件，通过事件匹配完成事件转移和结果返回，从而将整个流程衔接起来。
下面以一个HTTP请求的处理过程详述Vert.x的事件处理流程: Vert.x启动时，会将Worker Vertical的事件处理函数加载到Event Bus，当一个HTTP请求发送到Vert.x构建的应用时，Event Loop首先接收到请求，并对请求做分析、包装，然后将事件交给Event Bus来处理，Event Bus为此次请求事件添加一个事件ID，然后根据注册的Worker Vertical事件寻找已经注册的监听函数，若未找到则会抛弃该事件，若找到则会对处理类进行实例化，并同时使用事件ID在Event Bus中注册一个返回结果处理事件，该事件为Event Vertical类型。下一步由Worker Vertical实例执行事件处理函数，事件处理函数中通常包含业务处理、数据库操作等。Worker Vertical实例处理结束后，将返回结果和事件信息返回给Event Bus，Event Bus找到在其中注册的Event Vertical实例，然后将返回数据交给该实例处理，Event Vertical实例进一步处理数据并将结果返回给浏览器。
数据传递:
事件驱动的处理过程，数据传递是非常重要的，Vert.x支持任意对象的数据格式。但使用对象时经常会遇到序列化和载入类的问题，比如在使用Java对象的时候，这种情况下使用JSON会更方便，这也是Vert.x推荐采用的方式。
特点:
Vert.x的事件模型，有如下几个特点：
1. 非阻塞处理请求，异步执行阻塞程序，保证了请求处理的高效性。
2. 使用Event Bus事件总线来进行通讯，可以轻松编写出分布式、松耦合、高扩展性的程序。
3. 使用Event Bus事件总线是Vert.x真正实现了多语言支持的基础，已支持Java、JavaScript、Ruby、Python、Groovy、Clojure、Ceylon。

