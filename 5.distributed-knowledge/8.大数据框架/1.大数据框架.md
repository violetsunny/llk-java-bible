## Lambda
Lambda 架构是一种实时大数据处理框架。它的核心思想是，对于计算量过大或者计算过于复杂的问题，将其分为批处理层（Batch Layer）和快速处理层（Speed Layer），其中批处理层是在主数据集上的全量计算，而快速处理层则是对增量数据的计算。当这两者各自计算出结果后，服务层（Serving Layer）再将结果合并起来，为用户提供统一的数据查询接口，就可以得到最终的查询结果了。

## FlinkCDC
Flink的一个组件，它是一个基于流的数据集成工具，用于捕获数据库的变更数据。
实现原理：
我们以使用 Flink CDC 从 MySQL 中同步数据的情景，来讲解下 Flink CDC 的工作原理。
一般来说，Flink CDC 同步数据需要两个步骤：
第一步是将源数据库的数据全量同步到目标数据库；
第二步是跟随源数据库的 binlog 日志，将源数据库的所有变动，以增量数据的方式同步到目标数据库。

我们先来看将源数据库的数据全量同步到目标数据库的过程。Flink CDC 将这个过程称之为“快照”（sanpshot），具体步骤是这样的。
1.Flink CDC 会获取一个全局读锁（global read lock），从而阻塞其他客户端往数据库写入数据。不用担心这个锁定时间会很长，因为它马上就会在第 5 步中被释放掉。
2.启动一个可重复读语义（repeatable read semantics）的事务，从而确保后续在该事务内的所有“读”操作都是在“一致的快照”（consistent snapshot）中进行。这一步中“可重复读语义”以及后续步骤只涉及“读”操作是非常关键的。因为只有在“可重复读语义”且不存在“写”操作的情况下， MySQL 的“可重复读语义”级别事务才不会存在“幻读”现象，这样才能保证后续做“扫描”和读取 binlog 位置时，它们的数据和时间点是对得上的。这就是“一致的快照”的含义，它保证了同步到目标数据库中的数据是完整的，并且和源数据库中的数据是完全相同的，既不会多一条，也不会少一条。
3.读取当前 binlog 的位置。
4.读取 Flink CDC 配置指定的数据库和表定义(schema)。
5.释放步骤 1 中的全局读锁。这个时候其他的客户端就可以继续往数据库中写入数据了。从步骤 1 到步骤 5，Flink CDC 并没有做非常耗时的任务，所以全局锁定的时间很短，这样对业务运行的影响可以尽量降至最小。
6.将步骤 4 读取的数据库和表定义，作用到目标数据库上。
7.对数据库里的表进行全表扫描，将读取出的每条记录，都发送到目标数据库。
8.完成全表扫描后，提交（commit）步骤 2 时启动的可重复读语义事务。
9.将步骤 3 读取的 binlog 位置记录下来，表明本次数据全量同步过程（也就是“快照”）成功完成。后续做增量同步时，如果发现没有这个 binlog 位置记录，就意味着数据全量同步过程是失败的，可以重新再做一次步骤 1 到步骤 9，直到全量同步成功为止。

ETL 是 Extract（抽取）、Transform（转换）、Load（加载）的缩写，是一种用于数据集成和数据仓库建设的重要技术。

## Flink
最优秀的流计算框架，实时处理框架。数据流是有向无环图（DAG）
Flink 是一个主从（Master--JobManager/Worker--TaskManager）架构的分布式系统。
主节点负责调度流计算作业，管理和监控任务执行。当主节点从客户端接收到作业相关的 JAR 包和资源后，进行裁剪分析和优化，生成执行计划，也就是需要执行的任务，然后将相关的任务分配给各个 Worker，由 Worker 负责任务的具体执行。
keyedSream KeyedState
FlinkCDC进行数据同步，全量和增量同步数据，消费binlog

jobmanager（Master节点）  taskmanager（Worker节点） --> task slot
jobmanager相当于master节点基本一个，负责任务管理和资源管理。jobmanager和taskmanager之间通过actorsystem进行通信。taskmanager相当于slave节点负责具体任务和任务在每个节点上的资源申请和管理。taskmanager从jobmanager接收任务并使用slot资源启动task。
Flink执行过程：client将作业提交到jobmanager，jobmanager会根据资源情况申请需要的taskslot，当有足够的资源时（或者配置为无需等待足够资源），JobManager 会将任务分配给 TaskManager 执行。TaskManager 接收到任务后，会为每个任务创建一个 Task，然后在分配的 Slots 中执行这些任务。
进程上并行，可以独占cpu和内存，分为多个taskmanager 。线程上并行分为多个task slot共享资源。
数据流：一组不会停止的数据记录流，而转换（Transformation）是将一个或多个流作为输入，从而生成一个或多个输出流。

1.数据并行处理：对于一个大规模的数据集，Flink 可以将其分割成多个分区，然后在多个计算节点上同时处理这些分区，从而提高处理速度。
2.流处理引擎：Flink 的核心是一个流处理引擎，它可以实时处理无边界的数据流。在处理实时数据时，流处理引擎采用了基于事件时间的处理方式，能够准确地处理乱序事件，并保证结果的准确性和一致性。
3.批处理引擎：在处理大规模的离线数据分析任务时，Flink 可以使用批处理引擎快速地处理数据集，并提供高效的迭代计算支持，从而提高分析效率。
4.状态管理：强大的状态管理功能，能够在流处理和批处理中有效地管理和维护状态数据。状态数据可以是任何类型的数据，例如计数器、累加器、列表、映射等。例如实时销售额、实时用户数等。
5.窗口操作：用于对数据流进行分组和聚合。滚动窗口，滑动窗口，会话窗口。窗口操作可以基于时间、计数、会话等多种方式进行聚合统计，能够满足不同的业务需求。例如按时间窗口统计实时销售额、按用户会话窗口统计用户行为等。
6.背压：通过分布式阻塞队列。
7.exactly-once: 两阶段提交
8.时间分类：事件时间（Event time）,摄入时间（Ingestion Time）,处理时间（Processing Time）.
9.水印：为了解决数据乱序，确保数据的准确性和一致性。当event time<水印时间t，则表示窗口结束时间和t相同的时间窗口触发进行计算。
10.内存管理：Network Buffers,Memory Manage poll,User Code。MemorySegment：Buffer(TaskManager之间进行数据传输),StreamRecord(java对象和Buffer之间转换)
11.算子链：将operator的sub task一起形成task，一个task在一个线程中执行。
12.分布式快照：将数据流和操作算子状态进行一致性快照。

吞吐下降：
1.源头消费延迟：部分算子长期busy状态，部分算子长期背压
2.chenckpoints失败：逻辑太大，部分算子长期背压
3.任务失败，重试：OOM
原因：
1.数据和计算出现倾斜，导致部分算子长期busy状态，部分算子长期背压。解决：先key上做分割，然后聚合计算后，再进行二次聚合计算。就是为了让数据的分配更加均匀。数据倾斜一般都是设置key和分组时造成热点数据过于集中造成的。
2.网络和第三方接口耗时过高。解决：提高请求效率，加快响应速度，提高并发性能。
3.状态过大或序列化慢。解决：序列化不要太复杂。

利用filter算子来剔除不符合规则的数据；
调整 Flink 的并行度、任务槽数量、内存大小等参数，以提高资源利用率和应用程序的性能；
合理地使用窗口的触发机制和清理策略，以提高性能和资源利用率；

## storm
最早的流计算框架
采用主从架构，主节点为 Nimbus，负责任务分配和监控；从节点为 Supervisor，负责管理 Worker 进程。每个 Worker 运行一个或多个 Task，处理数据流。数据流通过 Spout（数据源）和 Bolt（数据处理器）进行处理，形成一个有向无环图（DAG），称为 Topology。
Spout：从外部数据源（如 Kafka、数据库等）读取数据并发射到 Topology 中。例如，一个从 Kafka 中读取消息的 Spout 可以使用 Kafka 的消费者 API 来读取消息，并将消息发送到 Topology 中。
Bolt：处理接收到的数据，可以执行过滤、聚合、存储等操作。Storm 提供了状态管理的支持，允许 Bolt 保存和恢复操作状态。一个对数据进行过滤和转换的 Bolt 可以接收来自一个 Spout 的数据，对数据进行过滤和转换后发送到下一个 Bolt 中。
Tuple：Storm 中数据的基本单位，它表示一个数据项。Tuple 由多个字段组成，每个字段可以是任意类型的数据。Spout 和 Bolt 之间通过发送 Tuple 来进行数据的传递和处理。例如，一个表示用户行为的 Tuple 可以包含用户 ID、行为类型、行为时间等字段。

每个 Spout 和 Bolt 可以设置并行度，以提高处理能力。Storm 使用 Netty 作为底层消息队列，确保消息快速传递。
使用ZK进行协调集群状态

## spark
从批处理到流处理，内存计算和DAG简化处理流程，可扩展，高吞吐和错误容忍的流数据处理。本质上是将流数据分成一段段RDD块数据进行连续不断的批处理。伪实时，是批量处理，切分的越小越接近实时处理

SparkContext：它是与Spark集群交互的入口点，负责资源分配、任务调度等。
RDD（Resilient Distributed Dataset）：弹性分布式数据集，是Spark的基本数据结构，支持不可变、可分区、支持并行操作的数据集合。转换（Transformations）和行动（Actions）。
DAGScheduler：负责将用户程序转换为一个DAG（有向无环图），并将其分解为多个阶段（Stages）和任务（Tasks）。
TaskScheduler：负责在集群中的节点上实际调度和执行任务。
Executor：是在集群节点上运行的进程，负责执行任务并缓存数据。
Driver Program：包含用户定义的Spark作业逻辑，它创建SparkContext和DAG。

1.通过"速率控制器"来实现背压。使用了PID算法。
2.通过checkpoint(保存点)来实现失败的时候进行重新加载，保证数据不丢失，但是会重复。

工作原理
1.作业提交：用户通过Driver Program提交作业到SparkContext，SparkContext将作业提交给集群管理器（如Standalone Manager、YARN ResourceManager或Mesos）。
2.资源分配：集群管理器为Spark作业分配资源，启动Executor进程。
3.DAG构建：SparkContext接收到作业后，通过DAGScheduler构建出一个DAG，这个DAG表示了作业的执行计划。
4.任务划分：DAGScheduler将DAG分解为多个阶段，每个阶段包含一组任务。这些任务是并行执行的最小单元。
5.任务调度：TaskScheduler将任务分配给集群中的Executor执行。每个任务会处理一个或多个RDD的分区。
6.执行计算：Executor执行分配给它的任务，可能包括计算、数据转换和动作操作。计算结果可能会缓存在内存或磁盘中，以供后续操作使用。
7.结果返回：任务完成后，结果会返回给Driver Program，Driver Program可以进一步处理这些结果或将它们输出到外部存储系统。

## MapReduce
分为map（映射），shuffle（混洗），reduce（归约）三个部分。各map任务读入切分后的大规模数据进行处理并将数据作为一系列key:value对输出，输出的中间数据按照定义的方式通过shuffle程序分发到相应的reduce任务。Shuffle程序还会按照定义的方式对发送到一个reduce任务的数据进行排序。Reduce进行最后的数据处理。
1. Map阶段
   输入：MapReduce作业从输入源（如HDFS）读取数据，通常以键值对（key-value pairs）的形式组织。
   Map函数：用户编写的Map函数对输入数据进行处理。这个函数接收一个输入键值对，并产生一组中间键值对。Map函数可以输出多组中间数据，每组数据由一个键和一个值组成。
   分区：Map阶段的输出通常会根据键进行分区（partitioning），以便相同键的所有值都发送到同一个Reducer。分区策略可以是哈希分区或其他自定义分区方法。
2. Shuffle阶段
   排序：在Map任务和Reduce任务之间，MapReduce框架会执行一个称为Shuffle的过程。这个过程包括对Map输出的排序，确保所有相同键的值都被聚集在一起，并且按照键的顺序进行排序。
   传输：Shuffle过程还包括将排序后的数据传输给相应的Reduce任务。这个过程可能涉及跨网络的数据传输。
3. Reduce阶段
   输入：Reduce任务接收来自Map任务的数据，这些数据已经根据键进行了排序和分区。
   Reduce函数：用户编写的Reduce函数处理排序后的输入数据，并生成最终的输出。Reduce函数通常用于合并具有相同键的所有值，并生成一个输出值。
   输出：Reduce任务的输出是最终结果，通常写入到分布式文件系统（如HDFS）中。

MapReduce计算框架适用于超大规模的数据（100TB量级）且各数据之间相关性较低的情况。
MapReduce通过多JVM方式进行执行。

MapReduce的核心思想是将大规模的计算任务分解成小的、可并行处理的子任务，然后分布式地在多个节点上执行这些子任务，最后将结果合并。
切分聚合方式，也是分而治之思想。

## Samza
流计算从Kafka中取数据进行处理，将流程数据委托Kafka，将资源管理，任务调度和分布式执行等由YARN（Yet Another Resource Negotiator）分布式资源管理系统完成。

## Elastic-JOB
分布式任务调度框架
Elastic-Job 使用 Zookeeper 作为注册中心，负责服务的注册与发现，确保作业服务器和任务调度器之间的协调。
支持将作业分片，以便在多个节点上并行执行，提高处理效率。

## Camel
数据流转换通道框架：支持http kafka emq 等等