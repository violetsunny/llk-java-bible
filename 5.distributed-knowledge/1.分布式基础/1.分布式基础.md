## 架构设计的原则？
1.N+1设计。系统中的每个组件都应做到没有单点故障；
2.回滚设计。确保系统可以向前兼容，在系统升级时应能有办法回滚版本；
3.禁用设计。应该提供控制具体功能是否可用的配置，在系统出现故障时能够快速下线功能；
4.监控设计。在设计阶段就要考虑监控的手段；
5.多活数据中心设计。若系统需要极高的高可用，应考虑在多地实施数据中心进行多活，至少在一个机房断电的情况下系统依然可用；
6.采用成熟的技术。刚开发的或开源的技术往往存在很多隐藏的bug，出了问题没有商业支持可能会是一个灾难；
7.资源隔离设计。应避免单一业务占用全部资源；
8.架构应能水平扩展。系统只有做到能水平扩展，才能有效避免瓶颈问题；
9.非核心则购买。非核心功能若需要占用大量的研发资源才能解决，则考虑购买成熟的产品；
10.使用商用硬件。商用硬件能有效降低硬件故障的机率；
11.快速迭代。系统应该快速开发小功能模块，尽快上线进行验证，早日发现问题大大降低系统交付的风险；
12.无状态设计。服务接口应该做成无状态的，当前接口的访问不依赖于接口上次访问的状态。

## Serverless
Serverless的全称是Serverless computing无服务器运算，又被称为函数即服务（Function-as-a-Service，缩写为 FaaS），是云计算的一种模型。Serverless称为微服务运算，但不代表它真的不需要服务，而是说开发者再也不用过多考虑服务器的问题，计算资源作为服务而不是服务器的概念出现。Serverless是一种构建和管理基于微服务架构的技术，允许开发者在服务部署级别而不是服务器部署级别来管理应用部署，你甚至可以管理某个具体功能或端口的部署，以便让开发者快速迭代，更快速地开发软件。

函数即服务（Function as a Service，FaaS）是 Serverless 架构的一种实现方式，它允许开发人员将代码片段（函数）部署到云端，这些函数可以根据触发器（如 HTTP 请求或事件）自动执行。常见的 FaaS 平台包括 AWS Lambda、Google Cloud Functions 和 Microsoft Azure Functions。

## Service Mesh
服务网格化（Service Meshification）是一种将服务网格技术应用于现有的微服务架构的过程。服务网格化的目标是通过引入服务网格层来改进和优化服务之间的通信、监控和安全性。

服务网格化涉及以下几个关键概念：
1.透明代理：服务网格通过在每个服务实例旁边部署一个透明代理（通常称为 sidecar 代理）来实现。这些代理负责拦截服务之间的通信，从而使服务网格能够管理和监控流量。
2.流量管理：服务网格化允许开发人员和运维人员更好地控制服务之间的流量，例如实现负载均衡、故障恢复和流量分割等功能。
3.可观察性：服务网格化提供了对服务之间通信的详细监控和度量，使得问题诊断和性能分析变得更加容易。
4.安全性：服务网格化可以提高服务之间通信的安全性，例如通过自动加密通信、实现访问控制策略等。

通过服务网格化，开发人员和运维人员可以更好地管理和监控微服务架构，从而提高系统的可靠性、安全性和可维护性。常见的服务网格技术包括 Istio、Linkerd 和 Consul Connect。

## 高并发
“高并发”其实是我们要解决的问题，而“异步”则是为了更有效地利用 CPU 和 IO 资源，来解决“高并发”问题时的编程方式。

处理了12亿业务数据的性能提升，日增120万，使其能够承载瞬时1万6的qps
（1）根据userId分为4库32表，使用的userId范围取模分库，userId % 128，0-31在0库 .... ，迁库先提前刷部分数据，然后上线后双写，然后再刷双写前这中间的数据，对比两边库的数据量后，再切新库。
（2）在数据保存时，增加redis对部分热点key做缓冲，redis热点key的超时时间固定时间+范围内随机时间。考虑用caffeine加本地缓存
（3）优化sql,比如索引的使用，MAX(id)比order by  id desc limit 1 性能好。
（4）接入点火，对热点服务接口预热，如将数据库连接池等第一次加载耗时或懒加载的服务先启动。
（5）其他代码细节注意，如减少业务的复杂度(异步流程使用)，线程池的使用避免过多线程造成线程频繁切换，减少日志的打印，BeanUtils的使用，增加限流和熔断机制等等。

## CAP原则
1.Consistency:一致性，是指所有节点在同一时刻看到的数据完全相同。
2.Availability:可用性，是指系统在任何时候都能够正常响应客户端的请求。
3.Partition tolerance:分区容错性，是指系统各个节点出现网络分区（就是节点之间失去联系）的情况下能够正常工作。

CA就是本地服务，CP就是分布式一致性服务ZK，AP就是分布式弱一致性服务MQ,Redis

BASE原则：
Basically Available(基本可用，允许部分功能失效)：就是在高并发大流量时，允许部分非核心服务不可用，更资源倾斜核心服务。
Soft state(软状态，中间状态)：就是允许服务多节点之间暂时处于不一致的状态，仍然能够提供服务。降低强一致性。
Eventually consistent (最终一致性)：就是服务之间不保证强一致，但是要最终一定是一致。增强可用性。

Base原则是分布式服务在一致性和可用性之间选择的一个平衡，但都是要保证分区容错性的基础上。

## 负载均衡算法
1、轮询/加权轮询
3、源地址哈希(ip、url)
4、随机/加权随机
6、最小连接数

拉出流量是在负载均衡里做的。让服务不可用踢出可使用列表。

## 雪花算法
先说第一位，long类型的数字是有符号的，第一位是0就是正数，1是负数，我们生成的id必然要正数，所以第一位可以确定为0不变。

剩下的63位分成三大部分，第一部分占41位，用来表示时间。第二部分占10位，用来表示程序所在机器的信息，剩下的第三部分占12位，填充的是一个序列，可以理解为递增数字，这样前面的时间和机器数字如果都一样的话，用序列的区别来生成不同的id。时间到毫秒，1024个机器，4096个序列

会有时钟回拨的问题。（1）时钟容忍，lock.park睡一个时间。（2）1024个机器做主备，进行切换

编号有序的方式：编号+年月日时分秒+同步生成的多位的升序数字
由于时间和数字都是有序的，所以整体也是有序的。

## 蝴蝶算法
对雪花算法的优化
Butterfly蝴蝶算法方案
对于以上三个问题，我们这里简述下我们的方案。
1.时间回拨问题
这里采用新的方案：大概思路是：启动时间戳采用的是“历史时间”，每次请求只增序列值，序列值增满，然后“历史之间”增1，序列值重新计算。
2.机器id分配和回收
这里机器id分配和回收具体有两种方案：zookeeper和db。理论上分配方案zk是通过哈希和扩容机器，而db是通过查找机制。回收方案，zk采用的是永久节点，节点中存储下次过期时间，客户端定时上报（设置心跳），db是添加过期时间字段，查找时候判断过期字段。
3.机器id上限
这个采用将改造版雪花+zookeeper分配id方案作为服务端的节点，客户端采用双Buffer+异步获取提高性能，服务端采用每次请求时间戳增1。

## 分布式事务
1.2pc  1.协调者询问是否可以执行，执行后不提交，反馈执行结果给协调者。2.协调者得到所有执行成功后再通知提交，否则通知回滚。
2.3pc  询问是否可以提交，执行后预提交，都可以再提交。
3.mq 生产者先发消息，然后commit提交通知，mqservice可以让消费者消费，rollback的话会删除消息，无响应会回查生产者状态。

两阶段提交（2PC，Two-Phase Commit）和三阶段提交（3PC，Three-Phase Commit）是分布式系统中确保事务在所有相关节点上保持一致性的协议。它们主要用于在分布式数据库系统中处理事务，以确保所有参与事务的节点要么都提交事务，要么都不提交，从而保持数据的一致性。

一、两阶段提交（2PC）
（1）阶段一：准备阶段（Prepare Phase）
协调者向所有参与者发送事务请求，并询问它们是否可以提交事务。
参与者收到请求后，执行事务操作，但不提交，将事务的执行结果记录下来，并向协调者回复是否可以提交事务。
如果参与者在执行事务过程中出现错误，它会回复协调者不能提交事务，并回滚已经执行的事务操作。
（2）阶段二：提交阶段（Commit Phase）
如果协调者收到所有参与者的回复都是可以提交事务，那么它会向所有参与者发送提交事务的请求。
参与者收到提交请求后，正式提交事务，并向协调者回复提交成功。
如果协调者收到任何一个参与者的回复是不能提交事务，那么它会向所有参与者发送回滚事务的请求。
参与者收到回滚请求后，回滚已经执行的事务操作，并向协调者回复回滚成功。

优点：
原理简单，实现相对容易。
能够在一定程度上保证分布式事务的原子性，即要么所有参与者都提交事务，要么所有参与者都回滚事务。
缺点：
同步阻塞问题：在准备阶段和提交阶段，所有参与者都处于阻塞状态，等待协调者的指令。如果协调者出现故障，那么所有参与者都将无法继续进行事务操作，直到协调者恢复正常。
单点故障问题：协调者是整个事务的核心，如果协调者出现故障，那么整个事务将无法继续进行。
数据不一致问题：在网络分区等情况下，可能会出现部分参与者收到提交请求，而部分参与者收到回滚请求的情况，导致数据不一致。

二、三阶段提交（3PC）
（1）阶段一：CanCommit 阶段（询问阶段）
协调者向所有参与者发送事务请求询问是否可以执行事务操作，并等待参与者的回复。
参与者收到请求后，根据自身情况判断是否可以执行事务操作，并向协调者回复是否可以执行事务操作。
（2）阶段二：PreCommit 阶段（准备阶段）
如果协调者收到所有参与者的回复都是可以执行事务操作，那么它会向所有参与者发送预提交请求。
参与者收到预提交请求后，执行事务操作，并将事务的执行结果记录下来，但不提交事务，然后向协调者回复预提交成功。
如果协调者收到任何一个参与者的回复是不能执行事务操作，那么它会向所有参与者发送中断事务的请求。
参与者收到中断事务的请求后，回滚已经执行的事务操作，并向协调者回复中断事务成功。
（3）阶段三：DoCommit 阶段（提交阶段）
如果协调者收到所有参与者的回复都是预提交成功，那么它会向所有参与者发送提交事务的请求。
参与者收到提交请求后，正式提交事务，并向协调者回复提交成功。
如果协调者在一定时间内没有收到所有参与者的回复，或者收到任何一个参与者的回复是不能提交事务，那么它会向所有参与者发送中断事务的请求。
参与者收到中断事务的请求后，回滚已经执行的事务操作，并向协调者回复中断事务成功。

优点：
相比两阶段提交，三阶段提交在一定程度上解决了同步阻塞问题。在 CanCommit 阶段和 PreCommit 阶段，参与者不会一直处于阻塞状态，而是可以在等待协调者指令的同时，继续处理其他事务操作。
降低了单点故障的影响。如果协调者在 DoCommit 阶段出现故障，那么参与者可以根据自身的事务状态进行提交或回滚操作，而不需要一直等待协调者的指令。
缺点：
仍然存在数据不一致的风险。在网络分区等情况下，可能会出现部分参与者收到提交请求，而部分参与者收到中断事务请求的情况，导致数据不一致。
实现相对复杂，需要更多的通信和协调开销。

总的来说，三阶段提交通过增加一个阶段来改进两阶段提交的一些问题，但同时也增加了协议的复杂性。通过一起执行和一起回滚来相对保证原子性。

## Sagas/TCC
分布式事务
TCC :Try-Confirm-Cancel + 协调者（二阶段优化版）
先是服务调用链路依次执行 Try 逻辑，就是锁定资源，检查预留是否满足。
如果Try都正常的话，TCC 就会完成业务逻辑，确认预留的资源。
如果Try有异常则进行释放已有的资源，或者进行补偿操作。
ByteTCC，TCC-transaction，Himly
缺点：很多业务场景不能用TCC定义，要同时实现Try,Confirm,Cancel三个接口。
Sagas:confirm-cancel  以数据库维度操作
优点：适用于数据一致性要求比较高的场景，由于采用补偿机制(正序或反序补偿)，每个原子操作都是先执行任务避免资源锁定，实时释放性能相对保障。
缺点：不能保证隔离性，会有更新丢失，脏读等问题，需要业务层控制并发，业务实现过于复杂。
Calvin事务模型是一种集中化的事务处理模式，能在高竞争环境下具有高吞吐量的能力，其中关键就是通过重新调度事务的执行，消除竞争。
去中心化事务：MVCC,2PC,PaxosGroup 快照隔离
如果业务对强一致性要求不是那么高那么最终一致性则是一种比较好的方案。
通常的做法就是补偿，比如 一个业务是 A 调用 B，两个执行成功才算最终成功，当 A 成功之后，B 执行失败如何来通知 A 呢。
比较常见的做法是 失败时 B 通过 MQ 将消息告诉 A，A 再来进行回滚。这种的前提是 A 的回滚操作得是幂等的，不然 B 重复发消息就会出现问题。

## 分布式锁
1.redis  （redisson）set ex nx(在key不存在时操作加锁，按秒超时)
2.zk 顺序性和一致性 利用临时节点
3.数据库 一个请求key(雪花算法)，监听着key前面的值，到自己了执行，执行完成删除。也可以是更新同一个key,触发行锁。可以实现Lock接口

redis极端情况也是锁不住的，当master挂了，其他slave并没有加锁，会出现漏洞。
zk是大部分机器加锁了才能执行。但zk效率没有redis好。 redis也用redlock进行保证，消耗大。

超时，程序没运行完，还是有问题。
Redisson可以每隔三分之一时间检查是否执行完（检查key是否还在），没执行完进行续期。
进行业务分段加锁，可以增加同一个数据的并发处理。
超时时间需要看业务情况了，可能需要给重试机会。

## 怎么做幂等
多次相同的调用返回相同的结果。
0.业务唯一性(订单号)+状态机
1.分布式锁，锁住的是多步操作让业务执行一次。
2.幂等表，操作时触发行锁。幂等表的问题：1.不符合幂等定义 2.唯一索引不在唯一后兼容有问题。
3.数据库唯一索引
4.token机制，防止重复提交
5.本地做，本地缓存/乐观锁 CAS
6.重试，不过不能影响结果

## 线上如何做压测、抗压手段
满负荷跑服务器和数据库
高并发，大数量请求

## 秒杀模块怎么设计
前端限制访问请求量，做倒计时控制，防止重复请求，请求后端接口。
后端支持内存级的请求数量限制，可通过工作台配置，超过的驳回，通过的继续请求，后端采用redis管理资源，抢占资源，未抢到的驳回，抢到的发送mq消息异步处理。
后端订单服务收到mq消息，开始创建订单，并且提醒用户付款，用户付款后开始，继续下单流程，处理扣库存操作，没有库存返回失败。

token身份效验，幂等 Redis数据缓存。不能直接更新数据库，可以先更新缓存，插入日志备份。在更新数据库。实时和定时缓存和数据库的一致性。黑白名单过滤请求。可以生成预订单，无状态订单，超过的部分进行删除。
分布式锁，分布式事务  订单号用雪花算法
MQ做削峰处理，分发到各个机器处理订单后续流程。
dubbo请求可以做熔断，限流，优先级，服务降级等
一定要做监控和预警

秒杀还是对某些重要资源的抢夺。那就可以利用redis缓存先预存固定资源数据，然后通过限流加分布式锁的方式进行争抢资源。对于业务的后续流程可以进行异步操作和分布式事务共同协调的方式进行。同时如果压力很大的话，可以考虑业务上做预售，服务上对非核心进行降级，以预留服务空间。同时做好监控预警等。

## 常见的限流
限流是限制到达系统的并发请求数量，保证系统能够正常响应部分用户请求，而对于超过限制的流量，则通过拒绝服务的方式保证整体系统的可用性。

1.计数器方式 临界点问题，1分钟100的请求，在最后一秒100  优化是滑动窗口算法
2.令牌桶  阻塞队列，控制放入令牌的时序达到获取令牌的快慢
3.漏桶算法  阻塞队列，请求任意进入，但是流出的速度固定。

setinal使用的LeapArray：一个环形数组来实现滑动窗口，每个数组元素代表一个时间窗口（Bucket），CAS。

漏桶算法
漏桶算法思路很简单，我们把水比作是请求，漏桶比作是系统处理能力极限，水先进入到漏桶里，漏桶里的水按一定速率流出，当流出的速率小于流入的速率时，由于漏桶容量有限，后续进入的水直接溢出（拒绝请求），以此实现限流。
令牌桶算法
令牌桶算法的原理也比较简单，我们可以理解成医院的挂号看病，只有拿到号以后才可以进行诊病。
系统会维护一个令牌（token）桶，以一个恒定的速度往桶里放入令牌（token），这时如果有请求进来想要被处理，则需要先从桶里获取一个令牌（token），当桶里没有令牌（token）可取时，则该请求将被拒绝服务。令牌桶算法通过控制桶的容量、发放令牌的速率，来达到对请求的限制。

Redis 的计数器，Lua 执行的原子性，进行分布式限流：
````redis
local key = "rate.limit:" .. KEYS[1] --限流KEY
local limit = tonumber(ARGV[1])        --限流大小
local current = tonumber(redis.call('get', key) or "0")
if current + 1 > limit then --如果超出限流大小
return 0
else  --请求数+1，并设置1秒过期
redis.call("INCRBY", key,"1")
redis.call("expire", key,"1")
return current + 1
end
````

1.Guava RateLimiter：非常轻量级，性能很好，但功能相对简单。如果你的需求比较简单，只需要一个简单的限流器，那么Guava RateLimiter是一个不错的选择。
2.Resilience4j：除了限流器，还提供了熔断器，重试，隔离等多种容错策略。如果你需要这些高级功能，那么Resilience4j可能更适合你。
3.Bucket4j：支持多种存储后端，可以满足分布式环境下的限流需求。如果你的应用是分布式的，那么Bucket4j可能更适合你。

## 服务降级和熔断
调用方由于对方接口长时间无响应被动做熔断。降级是主动不调用某个服务，比如非核心业务可以不调用只记录直接返回。

## 灰度
灰度发布，灰度配置。就是在分布式发布里，先找一定比例服务验证一下。减少事故影响。但同时也要求新旧服务是兼容的，状态没有差异的。

## 背压（Backpressure）
背压可以理解为，生产者可以感受到消费者反馈的消费压力，并根据压力进行动态调整生产速率。
高并发下的处理数据，能否背压是重要关键点。
大都是在大批量离线数据处理中使用。

## 流式处理
流是用DAG有向无环图表示，流程节点，分解聚合再分解聚合，达到并行快速处理事件的目的。本质是时间异步处理，形成流水线。

流处理就是连续、实时、并发和以逐条记录的方式处理数据的意思。

Source是流处理作业的起点，负责从外部系统或数据源读取数据，并将其引入到流处理系统中。
Sink是数据流的最终目的地，它是处理数据流中的一个算子，通常负责将数据写入外部存储系统或数据库。

## 服务网关
微服务网关作为微服务后端服务的统一入口，它可以统筹管理后端服务，主要分为数据平面和控制平面：

数据平面主要功能是接入用户的HTTP请求和微服务被拆分后的聚合。使用微服务网关统一对外暴露后端服务的API和契约，路由和过滤功能正是网关的核心能力模块。另外，微服务网关可以实现拦截机制和专注跨横切面的功能，包括协议转换、安全认证、熔断限流、灰度发布、日志管理、流量监控等。
控制平面主要功能是对后端服务做统一的管控和配置管理。例如，可以控制网关的弹性伸缩；可以统一下发配置；可以对网关服务添加标签；可以在微服务网关上通过配置Swagger功能统一将后端服务的API契约暴露给使用方，完成文档服务，提高工作效率和降低沟通成本。

1.路由功能：
路由是微服务网关的核心能力。通过路由功能微服务网关可以将请求转发到目标微服务。在微服务架构中，网关可以结合注册中心的动态服务发现，实现对后端服务的发现，调用方只需要知道网关对外暴露的服务API就可以透明地访问后端微服务。
2.负载均衡：
API网关结合负载均衡技术，利用Eureka或者Consul等服务发现工具，通过轮询、指定权重、IP地址哈希等机制实现下游服务的负载均衡。
3.统一鉴权：
一般而言，无论对内网还是外网的接口都需要做用户身份认证，而用户认证在一些规模较大的系统中都会采用统一的单点登录（Single Sign On）系统，如果每个微服务都要对接单点登录系统，那么显然比较浪费资源且开发效率低。API网关是统一管理安全性的绝佳场所，可以将认证的部分抽取到网关层，微服务系统无须关注认证的逻辑，只关注自身业务即可。
4.协议转换：
API网关的一大作用在于构建异构系统，API网关作为单一入口，通过协议转换整合后台基于REST、AMQP、Dubbo等不同风格和实现技术的微服务，面向Web Mobile、开放平台等特定客户端提供统一服务。
5.指标监控：
网关可以统计后端服务的请求次数，并且可以实时地更新当前的流量健康状态，可以对URL粒度的服务进行延迟统计，也可以使用Hystrix Dashboard查看后端服务的流量状态及是否有熔断发生。
6.限流熔断：
在某些场景下需要控制客户端的访问次数和访问频率，一些高并发系统有时还会有限流的需求。在网关上可以配置一个阈值，当请求数超过阈值时就直接返回错误而不继续访问后台服务。当出现流量洪峰或者后端服务出现延迟或故障时，网关能够主动进行熔断，保护后端服务，并保持前端用户体验良好。
7.黑白名单：
微服务网关可以使用系统黑名单，过滤HTTP请求特征，拦截异常客户端的请求，例如DDoS攻击等侵蚀带宽或资源迫使服务中断等行为，可以在网关层面进行拦截过滤。比较常见的拦截策略是根据IP地址增加黑名单。在存在鉴权管理的路由服务中可以通过设置白名单跳过鉴权管理而直接访问后端服务资源。
8.灰度发布：
微服务网关可以根据HTTP请求中的特殊标记和后端服务列表元数据标识进行流量控制，实现在用户无感知的情况下完成灰度发布。
9.流量染色：
和灰度发布的原理相似，网关可以根据HTTP请求的Host、Head、Agent等标识对请求进行染色，有了网关的流量染色功能，我们可以对服务后续的调用链路进行跟踪，对服务延迟及服务运行状况进行进一步的链路分析。
10.文档中心：
网关结合Swagger，可以将后端的微服务暴露给网关，网关作为统一的入口给接口的使用方提供查看后端服务的API规范，不需要知道每一个后端微服务的Swagger地址，这样网关起到了对后端API聚合的效果。
11.日志审计：
微服务网关可以作为统一的日志记录和收集器，对服务URL粒度的日志请求信息和响应信息进行拦截。

Spring Cloud GetWay
是Spring Cloud的一个全新的API网关项目，目的是为了替换掉Zuul1，它基于Spring5.0 + SpringBoot2.0 + WebFlux（基于⾼性能的Reactor模式响应式通信框架Netty，异步非阻塞模型）等技术开发，性能⾼于Zuul，官方测试，Spring Cloud GateWay是Zuul的1.6倍，旨在为微服务架构提供⼀种简单有效的统⼀的API路由管理⽅式。
Spring Cloud Gateway可以与Spring Cloud Discovery Client（如Eureka）、Ribbon、Hystrix等组件配合使用，实现路由转发、负载均衡、熔断、鉴权、路径重写、⽇志监控等，并且Gateway还内置了限流过滤器，实现了限流的功能。

## Drools
规则引擎，内存级

## WebFlux
基于⾼性能的Reactor模式响应式通信框架Netty，异步⾮阻塞模型，以事件为驱动。
编码和传统相差较大，改造比较繁琐，很多中间件和框架不支持。

比较适合作为加快响应速度的中间工具，不适合作为整体服务。
